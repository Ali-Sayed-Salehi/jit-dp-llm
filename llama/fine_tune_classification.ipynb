{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login as huggingface_hub_login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../secrets/.env\")\n",
    "\n",
    "hugging_face_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "huggingface_hub_login(hugging_face_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "!huggingface-cli download meta-llama/Meta-Llama-3-8B --local-dir ../LLMs/llama3-8b --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../LLMs/llama3-8b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", num_labels=2)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"../LLMs/llama3-8b\",\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and avaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:02<00:00,  3.93ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  2.54ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! 7444 training / 1862 evaluation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../datasets/dataset.jsonl\", split=\"train\")\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(dataset) * split_ratio)\n",
    "\n",
    "train_dataset = dataset.select(range(0, split_index))\n",
    "eval_dataset = dataset.select(range(split_index, len(dataset)))\n",
    "\n",
    "# Shuffle ONLY the training set\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "train_dataset.to_json(\"../datasets/train.jsonl\", orient=\"records\", lines=True)\n",
    "eval_dataset.to_json(\"../datasets/eval.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"✅ Done! {len(train_dataset)} training / {len(eval_dataset)} evaluation samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format and tokenize the dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7444/7444 [00:01<00:00, 3794.51 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3538\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3537\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mand\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3538\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n\u001b[32m   3539\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_writer.py:645\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m close_stream:\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/fsspec/implementations/local.py:449\u001b[39m, in \u001b[36mLocalFileOpener.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 122] Disk quota exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_for_classification\u001b[39m(example):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: example[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(example[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# Ensure label is integer 0 or 1\u001b[39;00m\n\u001b[32m      5\u001b[39m     }\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_formatted_dataset = \u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_for_classification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m eval_formatted_dataset = train_dataset.map(format_for_classification)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# def tokenize_class(example):\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#     encoding = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     encoding[\"labels\"] = example[\"label\"]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# train_tokenized_dataset = train_formatted_dataset.map(tokenize_class, batched=True)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# eval_tokenized_dataset = eval_formatted_dataset.map(tokenize_class, batched=True)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3074\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3069\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3070\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3071\u001b[39m         total=pbar_total,\n\u001b[32m   3072\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3073\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3074\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3543\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[32m   3542\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3543\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3544\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3545\u001b[39m         tmp_file.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_writer.py:640\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema:\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    642\u001b[39m     \u001b[38;5;28mself\u001b[39m.pa_writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/datasets/arrow_writer.py:442\u001b[39m, in \u001b[36mArrowWriter._build_writer\u001b[39m\u001b[34m(self, inferred_schema)\u001b[39m\n\u001b[32m    440\u001b[39m     schema = schema.with_metadata({})\n\u001b[32m    441\u001b[39m \u001b[38;5;28mself\u001b[39m._schema = schema\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m \u001b[38;5;28mself\u001b[39m.pa_writer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_WRITER_CLASS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/pyarrow/ipc.py:85\u001b[39m, in \u001b[36mRecordBatchStreamWriter.__init__\u001b[39m\u001b[34m(self, sink, schema, use_legacy_format, options)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sink, schema, *, use_legacy_format=\u001b[38;5;28;01mNone\u001b[39;00m, options=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     84\u001b[39m     options = _get_legacy_format_default(use_legacy_format, options)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43msink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/pyarrow/ipc.pxi:582\u001b[39m, in \u001b[36mpyarrow.lib._RecordBatchStreamWriter._open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/pyarrow/io.pxi:2214\u001b[39m, in \u001b[36mpyarrow.lib.get_writer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/pyarrow/io.pxi:232\u001b[39m, in \u001b[36mpyarrow.lib.NativeFile.get_output_stream\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/pyarrow/io.pxi:246\u001b[39m, in \u001b[36mpyarrow.lib.NativeFile._assert_writable\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/speed-scratch/a_s87063/repos/perf-pilot/venv/lib/python3.12/site-packages/pyarrow/io.pxi:237\u001b[39m, in \u001b[36mpyarrow.lib.NativeFile._assert_open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: I/O operation on closed file"
     ]
    }
   ],
   "source": [
    "def format_for_classification(example):\n",
    "    return {\n",
    "        \"text\": example['prompt'],\n",
    "        \"label\": int(example[\"response\"])\n",
    "    }\n",
    "\n",
    "train_formatted_dataset = train_dataset.map(format_for_classification)\n",
    "eval_formatted_dataset = train_dataset.map(format_for_classification)\n",
    "\n",
    "def tokenize_class(example):\n",
    "    encoding = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    encoding[\"labels\"] = example[\"label\"]\n",
    "    return encoding\n",
    "\n",
    "train_tokenized_dataset = train_formatted_dataset.map(tokenize_class, batched=True)\n",
    "eval_tokenized_dataset = eval_formatted_dataset.map(tokenize_class, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LoRA with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = np.exp(logits) / np.exp(logits).sum(-1, keepdims=True)  # softmax\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs[:, 1]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up TrainingArguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training/output/llama-classifier\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./training/logs\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=eval_tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../LLMs/finetuned/classification/model/llama3-8B\")\n",
    "tokenizer.save_pretrained(\"../LLMs/finetuned/classification/tokenizer/llama3-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(eval_tokenized_dataset)\n",
    "logits = predictions.predictions\n",
    "probs = np.exp(logits) / np.exp(logits).sum(-1, keepdims=True)\n",
    "\n",
    "print(\"Probabilities for class 1:\", probs[:, 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
